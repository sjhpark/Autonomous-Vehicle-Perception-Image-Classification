{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to the parent directory\n",
    "import sys\n",
    "sys.path.append('../') # let you import from parent directory\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "# Import .py files and their functions\n",
    "from bbox import rot, get_bbox\n",
    "from utils import write_paths, write_labels, write_rotation_vectors, \\\n",
    "                    write_centroids, write_sizes, write_clouds, write_camera_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect and save labels, centroids, point clouds, and camera matrices data\n",
    "path = 'trainval' # path to folder containing training snapshots, bbox.bin, cloud.bin, proj.bin\n",
    "\n",
    "# File Paths\n",
    "if not os.path.exists('./trainval/trainval_paths.npy'):\n",
    "    write_paths(path) # trainval_paths.npy\n",
    "\n",
    "if not os.path.exists('./test/test_paths.npy'):\n",
    "    write_paths('test') # test_paths.npy\n",
    "\n",
    "# Labels of Bounding Boxes\n",
    "if not os.path.exists('./trainval/trainval_labels.npy'):\n",
    "    write_labels(path) # trainval_labels.npy\n",
    "\n",
    "# Bounding Box Features - Rotation Vectors\n",
    "if not os.path.exists('./trainval/trainval_rotation_vectors.npy'):\n",
    "    write_rotation_vectors(path) # trainval_rotations.npy\n",
    "\n",
    "# Bounding Box Features - Centroids\n",
    "if not os.path.exists('./trainval/trainval_centroids.npy'):\n",
    "    write_centroids(path) # trainval_centroids.npy\n",
    "\n",
    "# Bounding Box Features - Sizes\n",
    "if not os.path.exists('./trainval/trainval_sizes.npy'):\n",
    "    write_sizes(path) # trainval_sizes.npy\n",
    "\n",
    "# Camera Matrices\n",
    "if not os.path.exists('./trainval/trainval_camera_matrices.npy'):\n",
    "    write_camera_matrices(path) # trainval_camera_matrices.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for training data\n",
    "train_root = './trainval/'\n",
    "test_root = './test/'\n",
    "\n",
    "# Load data\n",
    "train_images = glob('trainval/*/*_image.jpg') # Grab all the training snapshot image paths\n",
    "test_images = np.load('test/test_images.npy') # Grab all the test snapshot images\n",
    "train_paths = np.load(train_root + 'trainval_paths.npy') # each training data's path (for reference)\n",
    "test_paths = np.load(test_root + 'test_paths.npy') # each test data's path (for reference)\n",
    "labels = np.load(train_root + 'trainval_labels.npy')\n",
    "rotation_vectors = np.load(train_root + 'trainval_rotation_vectors.npy')\n",
    "centroids = np.load(train_root + 'trainval_centroids.npy')\n",
    "sizes = np.load(train_root + 'trainval_sizes.npy')\n",
    "camera_matrices = np.load(train_root + 'trainval_camera_matrices.npy', allow_pickle=True)\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crop_bbox(image, rotation_vector, centroid, size, camera_matrix):\n",
    "#     \"\"\"\n",
    "#     Convert 3D bounding box to 2D bounding box and crop the object from the image.\n",
    "#     \"\"\"\n",
    "\n",
    "#     proj = camera_matrix\n",
    "\n",
    "#     R = rot(rotation_vector)\n",
    "#     t = centroid\n",
    "#     sz = size\n",
    "\n",
    "#     vert_3D, edges = get_bbox(-sz / 2, sz / 2)\n",
    "\n",
    "#     vert_3D = R @ vert_3D + t[:, np.newaxis]\n",
    "\n",
    "#     vert_2D = proj @ np.vstack([vert_3D, np.ones(vert_3D.shape[1])])\n",
    "#     vert_2D = vert_2D / vert_2D[2, :]\n",
    "\n",
    "#     x_array = []\n",
    "#     y_array = []\n",
    "#     for e in edges.T:\n",
    "#         x_array.append(vert_2D[0, e])\n",
    "#         y_array.append(vert_2D[1, e])\n",
    "#         # plt.plot(vert_2D[0, e], vert_2D[1, e], 'r-')\n",
    "#     x_array = np.clip(np.array(x_array), 0, image.shape[1]-1) # clip to prevent bbox out of image\n",
    "#     y_array = np.clip(np.array(y_array), 0, image.shape[0]-1) # clip to prevent bbox out of image\n",
    "#     x_array = torch.from_numpy(x_array)\n",
    "#     y_array = torch.from_numpy(y_array)\n",
    "\n",
    "#     x_min = torch.min(x_array)\n",
    "#     x_max = torch.max(x_array)\n",
    "#     y_min = torch.min(y_array)\n",
    "#     y_max = torch.max(y_array)\n",
    "\n",
    "#     if x_min == x_max:\n",
    "#         x_min -= 1\n",
    "#         x_max += 1\n",
    "#     elif y_min == y_max:\n",
    "#         y_min -= 1\n",
    "#         y_max += 1\n",
    "\n",
    "#     object_image = image[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "#     object_image = Image.fromarray(object_image)\n",
    "#     object_image = object_image.resize((224, 224))\n",
    "#     # object_image = np.array(object_image)\n",
    "\n",
    "#     return object_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a custom dataset class\n",
    "# class VehicleDataset(Dataset):\n",
    "#     def __init__(self, images, rotation_vectors, centroids, sizes, camera_matrices, labels, paths):\n",
    "#         self.images = images\n",
    "#         self.rotation_vectors = rotation_vectors\n",
    "#         self.centroids = centroids\n",
    "#         self.sizes = sizes\n",
    "#         self.camera_matrices = camera_matrices\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image = plt.imread(self.images[idx])\n",
    "#         rotation_vector = self.rotation_vectors[idx]\n",
    "#         centroid = self.centroids[idx]\n",
    "#         size = self.sizes[idx]\n",
    "#         camera_matrix = self.camera_matrices[idx]\n",
    "#         label = self.labels[idx]\n",
    "\n",
    "#         cropped_image = crop_bbox(image, rotation_vector, centroid, size, camera_matrix)\n",
    "\n",
    "#         return cropped_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "# idxx = random.randint(0, len(train_images))\n",
    "# print(idxx)\n",
    "# zz=crop_bbox(plt.imread(train_images[idxx]), rotation_vectors[idxx], centroids[idxx], sizes[idxx], camera_matrices[idxx])\n",
    "# plt.imshow(plt.imread(train_images[idxx])) # image\n",
    "# # plt.plot(xx,yy) # 3D bounding box\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(zz) # cropped image based on 2D bounding box\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train + Val dataset\n",
    "# trainval_dataset = VehicleDataset(train_images, rotation_vectors, centroids, \n",
    "#                                   sizes, camera_matrices, labels, train_paths)\n",
    "\n",
    "# # Train Val Split\n",
    "# train_size = int(0.8 * len(trainval_dataset))\n",
    "# val_size = len(trainval_dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(trainval_dataset, [train_size, val_size])\n",
    "\n",
    "# # Extract features from the images using a pre-trained model\n",
    "# resnet = models.resnet18(pretrained=True)\n",
    "# resnet = torch.nn.Sequential(*(list(resnet.children())[:-1])) # Remove the last layer to get features\n",
    "# resnet = resnet.to(device)\n",
    "# resnet.eval()\n",
    "\n",
    "# transform_train = T.Compose([\n",
    "#     T.ToTensor(),\n",
    "#     T.RandomHorizontalFlip(),\n",
    "#     T.RandomRotation(20),\n",
    "#     T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# def extract_features_train(dataset):\n",
    "#     features = []\n",
    "#     labels = []\n",
    "#     with torch.no_grad():\n",
    "#         for img, label in tqdm(dataset, desc='Extracting features from training images'):\n",
    "#             img = transform_train(img)\n",
    "#             img = img.unsqueeze(0)\n",
    "#             img = img.to(device)\n",
    "#             feature = resnet(img).squeeze().cpu().numpy()\n",
    "#             features.append(feature)\n",
    "#             labels.append(label)\n",
    "#     return np.array(features), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = len(set(labels)) # number of classes\n",
    "\n",
    "# X_train, y_train = extract_features_train(train_dataset)\n",
    "# X_val, y_val = extract_features_train(val_dataset)\n",
    "\n",
    "# # Train a LightGBM model on the extracted features\n",
    "# train_data = lgb.Dataset(X_train, label=y_train)\n",
    "# params = {\n",
    "#     'objective': 'multiclass',\n",
    "#     'metric': 'multi_logloss',\n",
    "#     'num_class': num_classes, # Replace with the number of classes\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'learning_rate': 0.01,\n",
    "#     'feature_fraction': 0.9, # random fraction of features to use\n",
    "#     'bagging_fraction': 0.8, # random fraction of samples of data for each tree\n",
    "#     'bagging_freq': 5, # frequency of bagging\n",
    "#     'verbose': 1,\n",
    "#     'bagging_seed': 42, # random seed for reproducibility\n",
    "# }\n",
    "\n",
    "# # Train the model\n",
    "# model = lgb.train(params, train_data, num_boost_round=3000, \n",
    "#                   valid_sets=[train_data], early_stopping_rounds=10)\n",
    "\n",
    "# # Make predictions and evaluate the model\n",
    "# y_pred = model.predict(X_val)\n",
    "# y_pred = np.argmax(y_pred, axis=1)\n",
    "# accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "# print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\whdqk\\anaconda3\\envs\\sam\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\whdqk\\anaconda3\\envs\\sam\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 190/190 [09:31<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8, Loss: 0.9633, Accuracy: 63.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [08:51<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8, Loss: 0.8377, Accuracy: 63.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 190/190 [09:46<00:00,  3.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Personal Projects\\AV_Perception\\perception_modified_resnet.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 75>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Personal%20Projects/AV_Perception/perception_modified_resnet.ipynb#X11sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Personal%20Projects/AV_Perception/perception_modified_resnet.ipynb#X11sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Personal%20Projects/AV_Perception/perception_modified_resnet.ipynb#X11sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     \u001b[39mfor\u001b[39;00m input_batch, label_batch \u001b[39min\u001b[39;00m fine_tune_val_loader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Personal%20Projects/AV_Perception/perception_modified_resnet.ipynb#X11sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m         input_batch, label_batch \u001b[39m=\u001b[39m input_batch\u001b[39m.\u001b[39mto(device), label_batch\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Personal%20Projects/AV_Perception/perception_modified_resnet.ipynb#X11sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m         outputs \u001b[39m=\u001b[39m resnet(input_batch)\n",
      "File \u001b[1;32mc:\\Users\\whdqk\\anaconda3\\envs\\sam\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\whdqk\\anaconda3\\envs\\sam\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\whdqk\\anaconda3\\envs\\sam\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\whdqk\\anaconda3\\envs\\sam\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\whdqk\\anaconda3\\envs\\sam\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 295\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "\u001b[1;32md:\\Personal Projects\\AV_Perception\\perception_modified_resnet.ipynb Cell 9\u001b[0m in \u001b[0;36mVehicleFullImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Personal%20Projects/AV_Perception/perception_modified_resnet.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Personal%20Projects/AV_Perception/perception_modified_resnet.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     image \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39;49mimread(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimages[idx])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Personal%20Projects/AV_Perception/perception_modified_resnet.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Personal%20Projects/AV_Perception/perception_modified_resnet.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[idx]\n",
      "File \u001b[1;32mc:\\Users\\whdqk\\anaconda3\\envs\\sam\\lib\\site-packages\\matplotlib\\pyplot.py:2158\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   2156\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(matplotlib\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mimread)\n\u001b[0;32m   2157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimread\u001b[39m(fname, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 2158\u001b[0m     \u001b[39mreturn\u001b[39;00m matplotlib\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mimread(fname, \u001b[39mformat\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\whdqk\\anaconda3\\envs\\sam\\lib\\site-packages\\matplotlib\\image.py:1563\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   1559\u001b[0m             \u001b[39mreturn\u001b[39;00m imread(response, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39mext)\n\u001b[0;32m   1560\u001b[0m \u001b[39mwith\u001b[39;00m img_open(fname) \u001b[39mas\u001b[39;00m image:\n\u001b[0;32m   1561\u001b[0m     \u001b[39mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[0;32m   1562\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mPngImagePlugin\u001b[39m.\u001b[39mPngImageFile) \u001b[39melse\u001b[39;00m\n\u001b[1;32m-> 1563\u001b[0m             pil_to_array(image))\n",
      "File \u001b[1;32mc:\\Users\\whdqk\\anaconda3\\envs\\sam\\lib\\site-packages\\matplotlib\\image.py:1696\u001b[0m, in \u001b[0;36mpil_to_array\u001b[1;34m(pilImage)\u001b[0m\n\u001b[0;32m   1679\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1680\u001b[0m \u001b[39mLoad a `PIL image`_ and return it as a numpy int array.\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1692\u001b[0m \u001b[39m    - (M, N, 4) for RGBA images.\u001b[39;00m\n\u001b[0;32m   1693\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1694\u001b[0m \u001b[39mif\u001b[39;00m pilImage\u001b[39m.\u001b[39mmode \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mRGBA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRGBX\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mL\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m   1695\u001b[0m     \u001b[39m# return MxNx4 RGBA, MxNx3 RBA, or MxN luminance array\u001b[39;00m\n\u001b[1;32m-> 1696\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(pilImage)\n\u001b[0;32m   1697\u001b[0m \u001b[39melif\u001b[39;00m pilImage\u001b[39m.\u001b[39mmode\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mI;16\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m   1698\u001b[0m     \u001b[39m# return MxN luminance array of uint16\u001b[39;00m\n\u001b[0;32m   1699\u001b[0m     raw \u001b[39m=\u001b[39m pilImage\u001b[39m.\u001b[39mtobytes(\u001b[39m'\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m'\u001b[39m, pilImage\u001b[39m.\u001b[39mmode)\n",
      "File \u001b[1;32mc:\\Users\\whdqk\\anaconda3\\envs\\sam\\lib\\site-packages\\PIL\\Image.py:687\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    685\u001b[0m     new[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtobytes(\u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    686\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     new[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtobytes()\n\u001b[0;32m    688\u001b[0m \u001b[39mreturn\u001b[39;00m new\n",
      "File \u001b[1;32mc:\\Users\\whdqk\\anaconda3\\envs\\sam\\lib\\site-packages\\PIL\\Image.py:729\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[1;34m(self, encoder_name, *args)\u001b[0m\n\u001b[0;32m    726\u001b[0m \u001b[39mif\u001b[39;00m encoder_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m args \u001b[39m==\u001b[39m ():\n\u001b[0;32m    727\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\n\u001b[1;32m--> 729\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    731\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheight \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    732\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\whdqk\\anaconda3\\envs\\sam\\lib\\site-packages\\PIL\\ImageFile.py:257\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mimage file is truncated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    253\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(b)\u001b[39m}\u001b[39;00m\u001b[39m bytes not processed)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    254\u001b[0m         )\n\u001b[0;32m    256\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[1;32m--> 257\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    259\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class VehicleFullImageDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = plt.imread(self.images[idx])\n",
    "        image = Image.fromarray(image)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Create a new dataset for fine-tuning with full images\n",
    "fine_tune_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "fine_tune_dataset = VehicleFullImageDataset(train_images, labels, fine_tune_transform)\n",
    "\n",
    "# Split the new dataset into training and validation subsets\n",
    "train_size = int(0.8 * len(fine_tune_dataset))\n",
    "val_size = len(fine_tune_dataset) - train_size\n",
    "\n",
    "fine_tune_train_dataset, fine_tune_val_dataset = random_split(fine_tune_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders for the fine-tuning datasets\n",
    "batch_size = 32\n",
    "\n",
    "fine_tune_train_loader = DataLoader(fine_tune_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "fine_tune_val_loader = DataLoader(fine_tune_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Use the pre-trained ResNet model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "for module in resnet.modules():\n",
    "    if isinstance(module, nn.ReLU):\n",
    "        module.inplace = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n",
    "\n",
    "# Replace the last layer with a new fully connected layer for classification\n",
    "# num_classes = len(set(labels))\n",
    "# resnet.fc = torch.nn.Linear(resnet.fc.in_features, num_classes)\n",
    "\n",
    "# Add a few more FC layers to the model\n",
    "num_classes = len(set(labels))\n",
    "resnet.fc = nn.Sequential(\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128, num_classes)\n",
    ")\n",
    "\n",
    "# Freeze all layers except the last 5 layers\n",
    "for name, param in resnet.named_parameters():\n",
    "    if \"fc\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Set a smaller learning rate for fine-tuning\n",
    "fine_tune_lr = 0.001\n",
    "optimizer = optim.SGD(resnet.parameters(), lr=fine_tune_lr, momentum=0.9)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model for a few more epochs\n",
    "fine_tune_epochs = 8\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "for epoch in range(1, fine_tune_epochs + 1):\n",
    "    # Training loop\n",
    "    resnet.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (image_batch, label_batch) in tqdm(enumerate(fine_tune_train_loader, 1), total=len(fine_tune_train_loader)):\n",
    "        input_batch, label_batch = image_batch.to(device), label_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet(input_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    resnet.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for input_batch, label_batch in fine_tune_val_loader:\n",
    "            input_batch, label_batch = input_batch.to(device), label_batch.to(device)\n",
    "\n",
    "            outputs = resnet(input_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += label_batch.size(0)\n",
    "            correct += (predicted == label_batch).sum().item()\n",
    "\n",
    "    # Print the epoch summary\n",
    "    print(f\"Epoch {epoch}/{fine_tune_epochs}, \"\n",
    "          f\"Loss: {running_loss / len(fine_tune_train_loader):.4f}, \"\n",
    "          f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2631/2631 [04:06<00:00, 10.69it/s]\n",
      "Saving predictions to file: 2631it [00:00, 50730.32it/s]\n"
     ]
    }
   ],
   "source": [
    "class VehicleTestImageDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "test_batch_size = 1\n",
    "\n",
    "test_image_dataset = VehicleTestImageDataset(test_images, test_transform)\n",
    "test_image_loader = DataLoader(test_image_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "resnet.eval()\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs in tqdm(test_image_loader):\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        outputs = resnet(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        test_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "with open('test_predictions_modified_resnet.csv', 'w', newline='') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "\n",
    "    # Write the header\n",
    "    csv_writer.writerow([\"guid/image\", \"label\"])\n",
    "\n",
    "    for i, pred in tqdm(enumerate(test_predictions), desc='Saving predictions to file'):\n",
    "        csv_writer.writerow([test_paths[i], pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idxx = 414\n",
    "\n",
    "# xx=object_detection_model(transform_test(test_images[idxx]).unsqueeze(0))\n",
    "# xx[0]['boxes'].shape[0]\n",
    "# xx = xx[0]['boxes'][0]\n",
    "# x_min, y_min, x_max, y_max = map(int, xx)\n",
    "# plt.imshow(test_images[idxx][y_min:y_max, x_min:x_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e93292d3dc13a62b6b9bede236b6c227f26bb9f3b4f0e79effef0a290a3c3e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
